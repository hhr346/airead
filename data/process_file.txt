Stable Flow: Vital Layers for Training-Free Image Editing
OmriAvrahami1,2 OrPatashnik1,3 OhadFried4 EgorNemchinov1
KfirAberman1 DaniLischinski2 DanielCohen-Or1,3
1SnapResearch 2TheHebrewUniversityofJerusalem 3TelAvivUniversity 4ReichmanUniversity
Non-rigid editing Adding an object
“Jumping” “Sitting” “Sniffing the “Wearing “Wearing a “Next to an
road” green glasses” straw hat” avocado”
Object replacement Scene editing
“An otter” “A pig” “A plastic “On a wet “During the “Snowy day”
bag” road” evening”
Figure1. StableFlow. Ourtraining-freeeditingmethodisabletoperformvarioustypesofimageeditingoperations,includingnon-rigid
editing,objectaddition,objectremoval,andglobalsceneediting.Thesedifferenteditsaredoneusingthesamemechanism.
Abstract 1.Introduction
Overtherecentyears,wehavewitnessedanunprecedented
Diffusion models have revolutionized the field of content
explosionincreativeapplicationsofgenerativemodels,fu-
synthesisandediting. Recentmodelshavereplacedthetra-
eled by diffusion-based models [36, 78–80]. Recent mod-
ditional UNet architecture with the Diffusion Transformer
els, such as FLUX [46] and SD3 [25], have replaced the
(DiT), and employed flow-matching for improved training
traditionalUNetarchitecture[73]withtheDiffusionTrans-
andsampling. However,theyexhibitlimitedgenerationdi-
former(DiT)[63],andadoptedflowmatching[5,49,50]as
versity. In this work, we leverage this limitation to per-
asuperioralternativefortrainingandsampling.
formconsistentimageeditsviaselectiveinjectionofatten-
These flow-based models are based on optimal trans-
tionfeatures. Themainchallengeisthat, unliketheUNet-
port conditional probability paths, resulting in faster train-
based models, DiT lacks a coarse-to-fine synthesis struc-
ing and sampling, compared to diffusion models. This is
ture, making it unclear in which layers to perform the in-
attributed [49] to the fact that they follow straight line tra-
jection. Therefore, we propose an automatic method to
jectories,ratherthancurvedpaths. Oneoftheknowncon-
identify“vitallayers”withinDiT,crucialforimageforma-
sequencesofthisdifference, however, isthatthesemodels
tion,anddemonstratehowtheselayersfacilitatearangeof
exhibitlowerdiversitythanpreviousdiffusionmodels[26],
controlled stableedits,fromnon-rigidmodificationstoob-
asshowninFigure2(1-2).Whilereduceddiversityisgener-
ject addition, using the same mechanism. Next, to enable
allyconsideredanundesirablecharacteristic, inthispaper,
real-imageediting,weintroduceanimprovedimageinver-
wesuggestleveragingitforthetaskoftraining-freeimage
sion method for flow models. Finally, we evaluate our ap-
editing,asshowninFigure2(3)andFigure1.
proach through qualitative and quantitative comparisons,
Specifically, we explore image editing via parallel gen-
along with a user study, and demonstrate its effectiveness
eration [4, 18, 91], where features from the generative tra-
acrossmultipleapplications.
jectoryofthesource(reference)imageareinjectedintothe
trajectoryoftheeditedimage. Suchanapproachhasbeen
showneffectiveinthecontextofconvolutionalUNet-based
Project page is available at: https://omriavrahami.com/stable-flow ThisresearchwasperformedwhileOmriwasatSnap.
1]66[LXDS)1(
]64[XULF)2(
wolFelbatS)3(
Inordertosupporteditingrealimages,itistypicallynec-
essary to invert them first [53, 79]. We employ an Inverse
Euler Ordinary Differential Equation (ODE) solver to in-
vert real images in the FLUX model [46]. However, this
methodfailstoreconstructtheinputimageinasatisfactory
manner. Toimprovereconstructionaccuracy,weintroduce
an out-of-distribution (OOD) nudging technique, in which
weapplyasmallscalarperturbationtothecleanlatentbe-
foreinvertingit.WedemonstratethatthismakesFLUXless
pronetoundesiredchangesintheimageduringtheforward
pass.
Finally, we compare our method against its baselines
qualitativelyandquantitatively,andreaffirmtheresultswith
a user study. We also demonstrate several applications of
ourmethod.
“Aphotoofa +“dogwearing +“catwearing +“casting
dogandacat...” abluehat” yellowglasses” shadows” In summary, our contributions are: (1) we propose an
automatic method to detect the set of vital layers in DiT
Figure2. LeveragingReducedDiversity. Usingthesameinitial
modelsanddemonstratehowtousethemforimageediting;
seedwithdifferenteditingprompts,diffusionmodelssuchas(1)
(2)wearethefirstmethodtoharnessthelimiteddiversityof
SDXLgeneratediverseresults(differentidentitiesofthedogand
flow-basedmodelstoperformdifferentimageeditingtasks
thecat),while(2)FLUXgeneratesamorestable(lessdiverse)set
usingthesamemechanism;and(3)wepresentanextension
ofresultsout-of-the-box.However,therearestillsomeunintended
differences(thedogisstandingintheleftmostcolumnandsitting thatallowseditingrealimagesusingtheFLUXmodel.
intheothers,thecolorofthecatischanging,andtheroadisdif-
2.RelatedWork
ferentontheright).Usingourapproach,(3)StableFlow,theedits
arestable,maintainingconsistencyoftheunrelatedcontent.
Text-DrivenImageEditing. Aftertheemergenceoftext-
to-image models, many works have suggested using them
diffusion models [18], where the roles of the different at- for various applications [7, 12, 20, 37, 75], including text-
tention layers are well understood. However, such under- drivenimageeditingtasks[39,65]. SDEdit[52]addressed
standing has not yet emerged for DiT [63]. Specifically, the image-to-image translation task adding noise and de-
DiTdoesnotexhibitthesamefine-coarse-finestructureof noising with a new prompt. Blended Diffusion [8, 9] sug-
theUNet[63],henceitisnotclearwhichlayersshouldbe gested performing localized image editing [15, 38, 47, 55,
tamperedwithtoachievethedesirededitingbehavior. 62] incorporating an input mask into the diffusion process
To address this gap, we analyze the importance of the in a training-free manner, while GLIDE [54], ImagenEdi-
different components in the DiT architecture, in order to tor[89],andSmartBrush[93]offeredfine-tuningthemodel
determine the subset that should be injected while editing. on a given input mask. Other works suggested inferring
More specifically, we introduce an automatic method for themaskfromtheinputimage[21,88]orviaanadditional
detectingasetofvitallayers—layersthatareessentialfor click input [69]. Other works [18, 34, 60, 84] offered to
theimageformation—bymeasuringthedeviationinimage injectinformationfromtheinputimageusingparallelgen-
contentresultingfrombypassingeachlayer. Weshowthat eration. While some methods [13, 45, 85] suggested fine-
thereisnosimplerelationshipbetweenthevitalityofalayer tuningthemodelper-image,arecentlineofworksuggested
and its position in the architecture, i.e., the vital layers are trainingadesignatedmodelonalargesyntheticdatasetfor
spreadacrossthetransformer. instruction-based edits [17, 76, 97]. However, none of the
Acloseexaminationofthevitallayerssuggeststhatthe abovemethodsisatraining-freemethodthatsupportsnon-
injectionoffeaturesintotheselayersstrikesagoodbalance rigid editing, object adding/replacement and scene editing
in the multimodal attention between the reference image altogether. Concurrency,Add-it[81]offersasolutiontothe
contentandtheeditingprompt. Consequently,limitingthe task of object addition using FLUX. Our method, on the
injectionoffeaturestoonlythevitallayerstendstoyielda otherhand,supportsothertypesofimageediting(e.g.,non-
stableedit, i.e., aneditthatchangesonlythepart(s)speci- rigidediting,objectreplacement).
fiedbythetextprompt,whileleavingtherestoftheimage
intact,asdemonstratedinFigure2(3). Wedemonstratethat Image Inversion. In the realm of generative models, in-
performingthesamefeatureinjection,enablesperforminga version [92] is the task of finding a code within the latent
varietyofimageedits,includingnon-rigidediting,addition spaceofagenerator[31,43,44]thatfaithfullyreconstructs
ofobjects,andscenechanges,asdemonstratedinFigure1. a given image. Initial methods were developed for GAN
2Generate Generate
)
ri
) s r
e
+ Layer N + Layer N e y a l
g
y n
a
l lla
( n o it a r e
n
++ LL aa yy ee rr i i -1 DINOv2 DINOv2 + LL aa yy ee rr i -i 1
is
s a
p y b ( n o it
a
e r
G
llu Perceptual Similarity
e n
e
g
F
+ Layer 1 + Layer 1
la
it
r
a
P
Text embeddings Visual embeddings Text embeddings Visual embeddings
Figure3. LayerRemoval. (Left)Text-to-imageDiTmodelsconsistofconsecutivelayersconnectedthroughresidualconnections[33].
Eachlayerimplementsamultimodaldiffusiontransformerblock[25]thatprocessesacombinedsequenceoftextandimageembeddings.
(Right)ForeachDiTlayer,weperformeanablationbybypassingthelayerusingitsresidualconnection.Then,wecomparethegenerated
resultontheablatedmodelwiththecompletemodelusingaperceptualsimilaritymetric.
models [1–3, 14, 24, 59, 64, 70, 71, 83, 95, 99, 100], and ing[18,30,84]throughtargetedmanipulationofUNetde-
more recently for diffusion-based models [16, 23, 29, 32, coderlayers[73].
40,51,53,58,79,87]. Inthiswork,wesuggestinvertinga Incontrast,state-of-the-arttext-to-imageDiT[63]mod-
realimageinflowmodelsusinglatentnudging,aswefound els(FLUX[46]andSD3[25])employafundamentallydif-
thatthestandardinverseODEsolverisinsufficient. ferentarchitecture,asshowninFigure3(left). Thesemod-
elsconsistofconsecutivelayersconnectedthroughresidual
3.Method connections [33], without convolutions. Each layer imple-
mentsamultimodaldiffusiontransformerblock[25](MM-
Our goal is to edit images based on text prompts while
DiT-Block)thatprocessesacombinedsequenceoftextand
faithfullypreservingtheuneditedregionsofthesourceim-
imageembeddings. UnlikeinUNets,therolesofthediffer-
age. Given an input image x and an editing prompt p, we
entlayersarenotyetintuitivelyclear,makingitchallenging
aim to generate a modified image xˆ that exhibits the de-
todeterminewhichlayersarebestsuitedforimageediting.
siredchangesspecifiedbypwhilemaintainingtheoriginal
To quantify layer importance in the FLUX model, we
contentelsewhere. Weleveragethelimiteddiversityofthe
devised a systematic evaluation approach. Using Chat-
FLUXmodelandfurtherconstrainittoenablesuchstable
GPT[56],weautomaticallygeneratedasetP ofk =64di-
imageeditsthroughselectiveinjectionofattentionfeatures
versetextprompts,anddrawasetSofrandomseeds. Each
ofthe sourceimageintothe processthatgenerates xˆ. Our
of these prompts was used to generate a reference image,
approachisdescribedinmoredetailbelow. InSection3.1,
yielding in a set G . For each DiT layer ℓ ∈ L, we per-
weevaluatelayerimportanceintheDiTmodelbyanalyzing ref
formedanablationbybypassingthelayerusingitsresidual
theperceptualimpactoflayerremoval(Figure3). Next,in
connection, as illustrated in Figure 3(right). This process
Section3.2,weemploythemostinfluentiallayers(termed
generated a set of images G from the same prompts and
vital layers) for image editing through attention injection. ℓ
seeds. Seethesupplementarymaterialformoredetails.
Finally,inSection3.3,weextendourmethodtorealimage
To assess the impact of each layer, we measured the
editing by inverting images into the latent space using the
perceptual similarity between G and G and using DI-
EulerinverseODEsolver,enhancedbylatentnudging. ref ℓ
NOv2[57](seeFigure3). Theresults,plottedinFigure4,
3.1.MeasuringtheImportanceofDiTLayers show that removing certain layers significantly affects the
generated images, while others have minimal impact. Im-
Recent text-to-image diffusion models [36, 66, 68, 72, 74]
portantly,influentiallayersaredistributedacrossthetrans-
predominantlyuseCNN-basedUNets, whichexhibitwell-
formerratherthanconcentratedinspecificregions. Wefor-
understoodlayerroles. Indiscriminativetasks,earlylayers
mallydefinethevitalityoflayerℓas:
detect simple features like edges, while deeper layers cap-
ture higher-level semantic concepts [77, 96]. Similarly, in 1 X
vitality(ℓ)=1− d(M (s,p),M (s,p)), (1)
generativemodels,early-middlelayersdetermineshapeand k full -ℓ
s∈S,p∈P
color, while deeper layers control finer details [43]. This
structurehasbeensuccessfullyexploitedintext-drivenedit- whereM representsthecompletemodel,M denotesthe
full -ℓ
30.95
0.9
0.85
0.8
0 5 10 15 20 25 30 35 40 45 50 55 60
LayerIndex
ytiralimiSlautpecreP
Figure 4. Layer Removal Quantitative Comparison. As ex-
plainedinSection3.1,wemeasuredtheeffectofremovingeach
layerofthemodelbycalculatingtheperceptualsimilaritybetween
thegeneratedimageswithandwithoutthislayer. Lowerpercep-
tual similarity indicates significant changes in the generated im-
ages(Figure5). Ascanbeseen, removingcertainlayerssignif-
icantly affects the generated images, while others have minimal
impact. Importantly, influential layers are distributed across the
transformerratherthanconcentratedinspecificregions.Notethat
the first vital layers were omitted for clarity (as their perceptual
similarityapproachedzero).
modelwithlayerℓomitted,andd(·,·)istheperceptualsim-
ilaritymetric. ThesetofvitallayersV isthendefinedas:
V ={ℓ∈L|vitality(ℓ)≥τ }, (2)
vit
whereτ isthevitalitythreshold.
vit
Figure 5 illustrates the qualitative differences between
vitalandnon-vitallayers. Whilebypassingnon-vitallayers
results in minor alterations, removing vital layers leads to
significantchanges:completenoisegeneration(G ),global
0
structureandidentitychanges(G ),andalterationsintex-
18
tureandfinedetails(G ). 56
3.2.ImageEditingusingVitalLayers
Givenasourceimagexgeneratedwithaknownseedsand
prompt p, we aim to modify p and generate an edited im-
age xˆ that exhibits the desired changes, while otherwise
preserving the source content. We adapt the self-attention
injectionmechanism,previouslyshowneffectiveforimage
and video editing [18, 91] in UNet-based diffusion mod-
els, to the DiT-based FLUX architecture. Since each DiT
layer processes a sequence of image and text embeddings,
weproposegeneratingbothxandxˆinparallelwhileselec-
tivelyreplacingtheimageembeddingsofxˆwiththoseofx,
butonlywithinthevitallayerssetV.
Remarkably,asshowninFigure1,thistraining-freeap-
proachsuccessfullyperformsdiverseeditingtasks, includ-
ingnon-rigiddeformations,objectaddition,objectreplace-
ment, and global modifications, all using the same set of
vitallayersV.
ferG
0G
5G
81G
25G
65G
Figure 5. Layer Removal Qualitative Comparison. As ex-
plainedinSection3.1,weillustratethequalitativedifferencesbe-
tweenvitalandnon-vitallayers. Whilebypassingnon-vitallay-
ers(G andG )resultsinminoralterations,bypassingvitallay-
5 52
ersleadstosignificantchanges: completenoisegeneration(G ),
0
globalstructureandidentitychanges(G ),andalterationsintex-
18
tureandfinedetails(G ). 56
To understand this effectiveness, we analyze the multi-
modalattentionpatternsinFLUX.Eachvisualtokensimul-
taneouslyattendstoallvisualandtexttokens,withattention
weights normalized across both modalities. Figure 6 con-
trasts the attention patterns in vital versus non-vital layers
attwokeypoints: ayellowpointinaregionthatshouldre-
mainunchanged(requiringcopyingfromthereferenceim-
age),andaredpointinanareatargetedforediting(requir-
ing generation based on the text prompt). In vital layers
(left), points meant to remain unchanged show dominant
attention to visual features, while points targeted for edit-
ing exhibit stronger attention to relevant text tokens (e.g.,
“avocado”). Conversely,non-vitallayers(right)showpre-
dominantly image-based attention even in regions marked
for editing. This suggests that injecting features into vital
layersstrikesagoodmultimodalattentionbalancebetween
preservingsourcecontentandincorporatingtextedits.
4Vital Non-Vital
Layers Layers
Input Output Input Output
a photo of a man holding an avocado a photo of a man holding an avocado a photo of a man holding an avocado a photo of a man holding an avocado
Figure6. Multi-ModalAttentionDistribution. Givenaninputimageofaman,weeditittoholdanavocadobyinjectingthereference
imageinthevitallayersonly(left)orinthenon-vitallayers(right),andvisualizethemultimodalattentionoftwopoints:ayellowpointina
regionthatshouldremainunchanged(requiringcopyingfromthereferenceimage),andaredpointinanareatargetedforediting(requiring
generationbasedonthetextprompt). Ascanbeseen,invitallayers(left),pointsmeanttoremainunchangedshowdominantattentionto
visualfeatures,whilepointstargetedforeditingexhibitstrongerattentiontorelevanttexttokens(e.g.,“avocado”). Conversely,non-vital
layers(right)showpredominantlyimage-basedattentioneveninregionsmarkedforediting.Thissuggeststhatinjectingfeaturesintovital
layersstrikesagoodmultimodalattentionbalancebetweenpreservingsourcecontentandincorporatingtext-guidedmodifications.
gnigduno/w)a(
gnigdunw)b(
To edit real images, we must first invert them into the
latent space, transforming samples from p to p . We ini-
1 0
tiallyimplementedaninverseEulerODEsolverforFLUX
byreversingthevectorfieldprediction. Giventheforward
Eulerstep:
z =z +(σ −σ )∗u (z ) (3)
t−1 t t+1 t t t
wherez representsthelatentattimestept,σ istheoptimal t t
transportstandarddeviationattimet,andu isthelearned
t
vectorfield,weproposedtheinversestep:
z =z +(σ −σ )∗u (z ) (4)
t t−1 t t+1 t t−1
assumingu (z )≈u (z )forsmallsteps.
Inputimage Reconstruction “Raisingitshand” t t t t−1
However, as Figure 7(a) demonstrates, this approach
Figure 7. Latent Nudging. As described in Section 3.3, when
provesinsufficientforFLUX,resultingincorruptedimage
inverting a real image, (a) a simple inverse Euler ODE solver
reconstructions and unintended modifications during edit-
leadstocorruptedimagereconstructionsandunintendedmodifi-
ing. Wehypothesizethattheassumptionu(z ) ≈ u(z )
cations during editing. On the other hand, (b) using our latent t t−1
nudgingtechniquesignificantlyreducesreconstructionerrorsand doesnothold,whichcausesthemodeltosignificantlyalter
betterconstrainseditstotheintendedregions. the image during the forward process. To address this, we
introducelatentnudging:multiplyingtheinitiallatentz by
0
3.3.LatentNudgingforRealImageEditing asmallscalarλ=1.15toslightlyoffsetitfromthetraining
distribution. Whilethismodificationisvisuallyimpercepti-
Flow models generate samples by matching a prior distri-
ble(Figure7(b)),itsignificantlyreducesreconstructioner-
butionp (Gaussiannoise)toadatadistributionp (theim-
0 1
agemanifold). InthespaceRd,wedefinetwokeycompo- rors and constrains edits to the intended regions. See the
nents: a probability density path p : [0,1]×Rd → R , supplementarymaterialformoredetails
t >0
which specifies time-dependent probability density func-
4.Experiments
R
tions ( p (x)dx = 1), and a vector field u : [0,1] ×
t t
Rd → Rd. This vector field generates a flow ϕ : [0,1]× InSection4.1wecompareourmethodagainstitsbaselines,
Rd →Rdthroughtheordinarydifferentialequation(ODE): both qualitatively and quantitatively. Next, in Section 4.2
dϕ (x) = u (ϕ (x));ϕ (x) = x. Transformingasample weconductauserstudyandreportresults. Furthermore,in
dt t t t 0
from p to a sample in p is achieved using ODE solvers Section4.3wepresenttheablationstudyresults. Finally,in
0 1
suchasEuler. Section4.4wedemonstrateseveralapplications.
5Table 1. Quantitative Comparison. We compare our method lines qualitatively on real images. As can be seen,
against the baselines in terms of text similarity (CLIP txt), im- SDEdit[52]hasdifficultymaintainingobjectidentitiesand
age similarity (CLIP ) and image-text direction similarity
img backgrounds. P2P+NTI [34, 53] struggles with preserv-
(CLIP ). Ascanbeseen,P2P+NTI[34,53],Instruct-P2P[17],
dir ingobjectidentitiesandwithaddingnewobjects. Instruct-
andMasaCTRL[18]sufferfromlowsimilaritytothetextprompt.
P2P [17] and MagicBrush [97] face challenges with non-
SDEdit[94]andMagicBrush[97]adheremoretothetextprompt,
rigid editing. MasaCTRL [18] struggles with preserving
but they struggle with image similarity and image-text direction
object identities and adding new objects. Our method, on
similarity. Ourmethod,ontheotherhand,achievesbetterimage
andimage-textdirectionsimilarity. theotherhand,isadheringtotheeditingpromptwhilepre-
servingtheidentities.
Method CLIP (↑) CLIP (↑) CLIP (↑) To quantify the performance of our method and the
txt img dir
baselines, we prepared an evaluation dataset based on
SDEdit[94] 0.24 0.71 0.07
COCO [48], that in contrast to previous benchmarks [76,
P2P+NTI[34,53] 0.21 0.76 0.08
Instruct-P2P[17] 0.22 0.87 0.07 97],alsocontainsnon-rigideditingtasks. Westartbyfilter-
MagicBrush[97] 0.24 0.88 0.11 ingthedatasetautomaticallytocontainatleastonepromi-
MasaCTRL[18] 0.20 0.76 0.03 nentnon-rigidbody. Next, foreachimage, weusevarious
StableFlow(ours) 0.23 0.92 0.14 imageeditingtasks(non-rigidediting, objectaddition, ob-
ject replacement and scene editing) that take into account
the prominent object, resulting in a total dataset of 3,200
Table2. AblationStudy. Weconductanablationstudyandfind
samples. Seethesupplementarymaterialformoredetails.
thatperformingattentioninjectioninallthelayersorperforming
Weevaluatedtheeditingresultsusingthreemetrics: (1)
an attention extension in all the layers significantly reduces the
textsimilarity. Furthermore,performinganattentioninjectionin CLIP img thatmeasuresthesimilaritybetweentheinputim-
the non-vital layers or removing the latent nudging reduces the age and the edited image, (2) CLIP txt that measure the
imagesimilarity. similarity between the edited image and the target editing
prompt, and (3) CLIP [28, 61] that measures the simi-
dir
Method CLIPtxt(↑) CLIPimg(↑) CLIPdir(↑) larity between the direction of the prompt change and the
StableFlow(ours) 0.23 0.92 0.14 directionoftheimagechange.
As can be seen in Table 1, P2P+NTI [34, 53], Instruct-
Injectionalllayers 0.17 0.98 0.00
Injectionnon-vitallayers 0.25 0.72 0.09 P2P[17],andMasaCTRL[18]sufferfromlowsimilarityto
Extensionalllayers 0.18 0.98 0.01 thetextprompt. SDEdit[94]andMagicBrush[97]adhere
w/olatentnudging 0.22 0.62 0.05
moretothetextprompt,buttheystrugglewithimagesim-
ilarityandimage-textdirectionsimilarity. Ourmethod, on
Table 3. User Study. We compare our method against the the other hand, is able to achieve better image and image-
baselinesusingthestandardtwo-alternativeforced-choiceformat. textdirectionsimilarity.
Userswereaskedtoratewhicheditingresultisbetter(Oursvs.the
baseline)intermsof:(1)targetpromptadherence,(2)inputimage 4.2.UserStudy
preservation,(3)realismand(4)overalleditquality.Wereportthe
WeconductanextensiveuserstudyusingtheAmazonMe-
winrateofourmethodcomparedtoeachbaseline.Asshown,our
methodoutperformsthebaselinesacrossallcategories,achieving chanical Turk (AMT) [6] platform, with the automatically
awinratehigherthantherandomchanceof50%. generated test examples from Section 4.1. We compare
all the baselines against our method using standard two-
Oursvs PromptAdher.(↑) ImagePres.(↑) Realism(↑) Overall(↑) alternative forced-choice format. Users were given the in-
SDEdit[52] 69.00% 68.00% 63.66% 70.66% putimage,theedittextprompt,andtwoeditingresults(one
P2P+NTI[34,53] 76.00% 71.00% 72.66% 65.33%
fromourmethodandonefromthebaseline).Foreachcom-
Instruct-P2P[17] 76.33% 75.66% 68.00% 60.33%
MagicBrush[97] 61.33% 67.33% 76.66% 74.00% parison,theuserswereaskedtoratewhicheditingresultis
MasaCTRL[18] 82.33% 80.00% 80.33% 72.00%
better in terms of: (1) target prompt adherence, (2) input
imagepreservation, (3)realismand(4)overalleditquality
4.1.QualitativeandQuantitativeComparison
(i.e.,whentakingallfactorsintoaccount).Ascanbeseenin
We compare our method against the most relevant Table3,ourmethodispreferredoverthebaselinesinover-
text-driven image editing methods. We re-implement alltermsaswellastheotherterms. Seethesupplementary
SDEdit [52] using the FLUX.1-dev [46] model, and use materialformoredetailsandstatisticalanalysis.
the official public implementations of P2P+NTI [34, 53],
4.3.AblationStudy
Instruct-P2P [17], MagicBrush [97] and MasaCTRL [18].
Seethesupplementarymaterialformoredetails. We conduct an ablation study for the following cases: (1)
In Figure 8 we compare our method against the base- Attentioninjectioninalllayers—weperformtheattention
6Input SDEdit[52] P2P+NTI[34,53] Instruct-P2P[17] MagicBrush[97] MasaCTRL[18] StableFlow(ours)
“Thecatisyellingandraisingitspaw”
“Arabbittoysittingandwearingpinksocksduringthelateafternoon”
“Arubberducknexttoapurpleballduringasunnyday”
“Adogwithasmallcollarliftingitspawwhilewearingredglasses”
“Abottlenexttoanapple.Thereisaheartpaintingonthewall.”
“Adollwithagreenbodywearingahat”
“Amanwithalonghair”
Figure8. QualitativeComparison. Wecompareourmethodonrealimagesagainstthebaselines. SDEdit[52]faceschallengeswith
preserving object identities and backgrounds (e.g., rabbit and cat examples). P2P+NTI [34, 53] struggles with both preserving object
identities(e.g.,rabbitandliondollsexamples)andaddingnewobjects(e.g.,missingballintheduckexampleandmissingheartinthe
bottleexample).Instruct-P2P[17]andMagicBrush[97]strugglewithperformingnon-rigidediting(e.g.,raisingofthepawsindogandcat
examples,andthesittingoftherabbitinitsexample). MasaCTRL[18]hasdifficultywithpreservingobjectidentities(e.g.,cat,dogand
liondollexamples)andaddingnewobjects(e.g.,missingballintheduckexampleandmissingsocksintherabbitexample).Ourmethod,
ontheotherhand,isabletoadheretotheeditingpromptwhilepreservingtheidentities.
7gnitidE.cnI)1(
Input “Holdinghands” “Wearingglasses” “Nexttoanalbino
porcupine”
elytS.tsnoC)2(
Input “StatueofLiberty” “TajMahal” “EiffelTower”
gnitidEtxeT)3(
Input “Manholdinga “Manholdinga “Manholdinga
signwiththe signwiththe signwiththe
text‘diffusion’ uppercasetext text‘Flow’”
inabluecolor” ‘DIFFUSION’”
Figure9. Applications. Ourmethodcanbeusedforvariousap-
plications:(1)IncrementalEditing—startingfromasceneoftwo
kids, theusercanrefinetheimageiterativelybymakingthekid
holdhands,thenwearglassesandfinallyaddaporcupinenextto
them. (2)ConsistentStyle—startingfromascenewithagiven
style,suchasananimationoftheGreatPyramidofGiza,theuser
can generate images of different places with the same style. (3)
TextEditing—givenascenethatcontainstext,ourmethodisable
toperformtext-relatededitingsuchascolorchange,casechange
andtextreplacement.
injectionthatisdescribedinSection3.2inallthelayers(in-
steadonthevitallayersonly). (2)Attentioninjectionnon-
vitallayers—weperformedtheattentioninjectioninsome
non-vitallayers(sameamountoflayersasvitallayers). (3)
Attention extension — instead of performing attention in-
jection as described in Section 3.2, we extended [35, 82]
theattentions.t. thegeneratedimagescanattendtotheref-
erenceimage,aswellasthemselves. (4)w/olatentnudging
—weomittedthelatentnudgingcomponent(Section3.3).
As can be seen in Table 2, we found that (1) perform-
ing attention injection in all the layers or performing (3)
an attention extensionin allthe layers, significantly harms
thetextsimilarity. Inaddition, (2)performinganattention
extensioninthenon-vitallayersor(4)removingthelatent
nudgingreducestheimagesimilarity.
4.4.Applications
As demonstrated in Figure 9, our method can be used for
various applications: (1) Incremental Editing — starting
fromagivenscene,theusercanrefinetheimageiteratively
in a step-by-step manner. (2) Consistent Style — starting
fromascenewithagivenstyle,theusercangenerateother
images in the same style [27, 35, 41]. (3) Text Editing —
gnitidEelytS)1(
Input “Animation” “Pencilsketch” “Oilpainting”
.garDtcejbO)2(
Input “Ontherightside “Ontheleftside “Onthebottom
oftheframe” oftheframe” oftheframe”
ecalpeR.gB)3(
Input “Intheforest” “Inthedesert” “Onthemoon”
Figure10. Limitations. Ourmethodsuffersfromthefollowing
limitations: (1)StyleEditing—givenaphotorealisticimageofa
boy,ourmethodstruggleswithchangingitsstyletoananimation
(the identity of the boy also changes), to pencil sketch (changes
onlytoblack&white)ortoanoilpainting(mainlymakestheim-
agesmoother). (2)ObjectDragging—givenanimageofacat,
ourmethodisunabletodragitintodifferentlocationsintheimage,
butchangesthegazeofthecatinstead. (3)BackgroundReplace-
ment—givenanimageofaratontheroad,ourmethodunableto
replaceitsbackgroundentirely(theroadleaks).
givenascenethatcontainstext, ourmethodisabletoper-
formtext-relatededitingsuchascolorchange,casechange
andtextreplacement.
5.LimitationsandConclusions
AsdemonstratedinFigure10,ourmethodsuffersfromthe
following limitations: (1) Style Editing — given an input
image in one style (e.g., photorealistic), our method strug-
gleswithchangingittoadifferentstyle(e.g.,oilpainting),
as it relies on attention injection (Section 3.2). (2) Object
Dragging — given an image with an object, our method
is unable to drag it [11] into different locations in the im-
age, as text-to-image models often struggle [10] with spa-
tial prompt adherence. (3) Background Replacement —
givenaninputimage, ourmethodstruggleswithreplacing
itsbackgroundentirelywithnoleakage[22].
In conclusion, we present Stable Flow, a training-free
method for image editing that enables various image edit-
ingtasksusingtheattentioninjectionofthesamevitallay-
ersgroup. Webelievethatourfully-automatedapproachof
detecting vital layers may be also beneficial for other use-
cases, such as generative models pruning and distillation.
We hope that our layer analysis will inspire more work in
thefieldofgenerativemodelsandexpandingthepossibili-
tiesforcreativeexpression.
8References mantic photo manipulation with a generative image prior.
ACMTransactionsonGraphics(TOG),38:1–11,2019. 3
[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
[15] DavidBau,AlexAndonian,AudreyCui,YeonHwanPark,
age2stylegan: Howtoembedimagesintothestyleganla-
AliJahanian,AudeOliva,andAntonioTorralba. Paintby
tentspace? InProceedingsoftheIEEE/CVFInternational
word. ArXiv,abs/2103.10951,2021. 2
ConferenceonComputerVision, pages4432–4441, 2019.
[16] Manuel Brack, Felix Friedrich, Katharina Kornmeier,
3
LinoyTsaban,PatrickSchramowski,KristianKersting,and
[2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
Apolin’arioPassos. Ledits++: Limitlessimageeditingus-
age2stylegan++: How to edit the embedded images? In
ingtext-to-imagemodels. 2024IEEE/CVFConferenceon
ProceedingsoftheIEEE/CVFconferenceoncomputervi-
Computer Vision and Pattern Recognition (CVPR), pages
sionandpatternrecognition,pages8296–8305,2020.
8861–8870,2023. 3
[3] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and
[17] Tim Brooks, Aleksander Holynski, and Alexei A. Efros.
AmitHaimBermano. Hyperstyle: Styleganinversionwith
Instructpix2pix: Learningtofollowimageeditinginstruc-
hypernetworks for real image editing. 2022 IEEE/CVF
tions. InCVPR,2023. 2,6,7,13,14,15,16
Conference on Computer Vision and Pattern Recognition
[18] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan,
(CVPR),pages18490–18500,2021. 3
Xiaohu Qie, and Yinqiang Zheng. MasaCtrl: tuning-free
[4] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar
mutual self-attention control for consistent image synthe-
Averbuch-Elor, andDanielCohen-Or. Cross-imageatten-
sis and editing. In Proceedings of the IEEE/CVF Inter-
tion for zero-shot appearance transfer. In International
national Conference on Computer Vision (ICCV), pages
Conference on Computer Graphics and Interactive Tech-
22560–22570,2023. 1,2,3,4,6,7,13,14,15,16
niques,2023. 1
[19] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve´ Je-
[5] Michael S Albergo and Eric Vanden-Eijnden. Building
gou,JulienMairal,PiotrBojanowski,andArmandJoulin.
normalizing flows with stochastic interpolants. ArXiv,
Emergingpropertiesinself-supervisedvisiontransformers.
abs/2209.15571,2022. 1
In2021IEEE/CVFInternationalConferenceonComputer
[6] Amazon. Amazon mechanical turk. https://www. Vision(ICCV),pages9630–9640,2021. 13,15,21,23
mturk.com/,2024. 6,14 [20] HilaChefer, ShiranZada, RoniPaiss, ArielEphrat, Omer
[7] MoabArar,AndreyVoynov,AmirHertz,OmriAvrahami, Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer
ShlomiFruchter, YaelPritch, DanielCohen-Or, andAriel Michaeli, and Inbar Mosseri. Still-moving: Customized
Shamir. Palp: Prompt aligned personalization of text-to- video generation without customized video data. ArXiv,
imagemodels. 2024. 2 abs/2407.08674,2024. 2
[8] OmriAvrahami,DaniLischinski,andOhadFried.Blended [21] GuillaumeCouairon,JakobVerbeek,HolgerSchwenk,and
diffusionfortext-driveneditingofnaturalimages. InPro- MatthieuCord. Diffedit: Diffusion-basedsemanticimage
ceedings of the IEEE/CVF Conference on Computer Vi- editingwithmaskguidance. InTheEleventhInternational
sionandPatternRecognition(CVPR),pages18208–18218, ConferenceonLearningRepresentations,2022. 2
2022. 2 [22] Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel
[9] OmriAvrahami,OhadFried,andDaniLischinski.Blended Cohen-Or. Be yourself: Bounded attention for multi-
latentdiffusion. ACMTrans.Graph.,42(4),2023. 2 subject text-to-image generation. ArXiv, abs/2403.16990,
[10] OmriAvrahami,ThomasHayes,OranGafni,SonalGupta, 2024. 8
YanivTaigman,DeviParikh,DaniLischinski,OhadFried, [23] GiladDeutch,RinonGal,DanielGaribi,OrPatashnik,and
and Xi Yin. Spatext: Spatio-textual representation for Daniel Cohen-Or. Turboedit: Text-based image editing
controllable image generation. In Proceedings of the using few-step diffusion models. ArXiv, abs/2408.00735,
IEEE/CVF Conference on Computer Vision and Pattern 2024. 3
Recognition(CVPR),pages18370–18380,2023. 8 [24] TanM.Dinh, A.Tran, RangHoManNguyen, andBinh-
[11] OmriAvrahami,RinonGal,GalChechik,OhadFried,Dani SonHua. Hyperinverter: Improvingstyleganinversionvia
Lischinski, Arash Vahdat, and Weili Nie. Diffuhaul: A hypernetwork. 2022IEEE/CVFConferenceonComputer
training-freemethodforobjectdragginginimages. arXiv Vision and Pattern Recognition (CVPR), pages 11379–
preprintarXiv:2406.01594,2024. 8,21,26 11388,2021. 3
[12] Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, [25] PatrickEsser,SumithKulal,A.Blattmann,RahimEntezari,
ShlomiFruchter, OhadFried, DanielCohen-Or, andDani Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz,
Lischinski. Thechosenone: Consistentcharactersintext- AxelSauer,FredericBoesel,DustinPodell,TimDockhorn,
to-imagediffusionmodels.InACMSIGGRAPH2024Con- ZionEnglish, KyleLacey, AlexGoodwin, YannikMarek,
ferencePapers,NewYork,NY,USA,2024.Associationfor andRobinRombach.Scalingrectifiedflowtransformersfor
ComputingMachinery. 2 high-resolution image synthesis. ArXiv, abs/2403.03206,
[13] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni 2024. 1,3,13,21,36
Kasten,andTaliDekel. Text2live: Text-drivenlayeredim- [26] Johannes S. Fischer, Ming Gui, Pingchuan Ma, Nick
ageandvideoediting. ArXiv,abs/2204.02491,2022. 2 Stracke, Stefan Andreas Baumann, Vincent Tao Hu, and
[14] David Bau, Hendrik Strobelt, William S. Peebles, Jonas BjornOmmer. Boostinglatentdiffusionwithflowmatch-
Wulff,BoleiZhou,Jun-YanZhu,andAntonioTorralba.Se- ing. ArXiv,abs/2312.07360,2023. 1
9[27] Yarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel [42] JustinJohnson, AlexandreAlahi, andLiFei-Fei. Percep-
Cohen-Or. Implicit style-content separation using b-lora. tuallossesforreal-timestyletransferandsuper-resolution.
ArXiv,abs/2403.14572,2024. 8 ArXiv,abs/1603.08155,2016. 15
[28] RinonGal,OrPatashnik,HaggaiMaron,AmitH.Bermano, [43] TeroKarras,SamuliLaine,andTimoAila. Astyle-based
GalChechik,andDanielCohen-Or. Stylegan-nada. ACM generatorarchitectureforgenerativeadversarialnetworks.
TransactionsonGraphics(TOG),41:1–13,2021. 6,14 InProceedingsoftheIEEEconferenceoncomputervision
[29] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar andpatternrecognition,pages4401–4410,2019. 2,3
Averbuch-Elor,andDanielCohen-Or.Renoise:Realimage [44] TeroKarras, SamuliLaine, MiikaAittala, JanneHellsten,
inversionthroughiterativenoising.ArXiv,abs/2403.14602, Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
2024. 3 ing the image quality of stylegan. In Proceedings of the
[30] MichalGeyer,OmerBar-Tal,ShaiBagon,andTaliDekel. IEEE/CVF Conference on Computer Vision and Pattern
Tokenflow: Consistent diffusion features for consistent Recognition,pages8110–8119,2020. 2
videoediting. arXivpreprintarXiv:2307.10373,2023. 3 [45] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-
[31] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing wen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani.
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Imagic: Text-basedrealimageeditingwithdiffusionmod-
andYoshuaBengio. Generativeadversarialnets. Advances els. InProceedingsoftheIEEE/CVFConferenceonCom-
inneuralinformationprocessingsystems,27,2014. 2 puter Vision and Pattern Recognition, pages 6007–6017,
[32] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kun- 2023. 2
peng Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, [46] Black Forest Labs. Flux. https://github.com/
DingLiu,QilongZhangli,AnastasisStathopoulos,Xiaox- black-forest-labs/flux,2024. 1,2,3,6,13,21
iao He, Jindong Jiang, Zhaoyang Xia, Akash Srivastava, [47] Shanglin Li, Bo-Wen Zeng, Yutang Feng, Sicheng Gao,
and Dimitris N. Metaxas. Proxedit: Improving tuning- Xuhui Liu, Jiaming Liu, Li Lin, Xu Tang, Yao Hu,
free real image editing with proximal guidance. 2024 Jianzhuang Liu, and Baochang Zhang. Zone: Zero-shot
IEEE/CVFWinterConferenceonApplicationsofComputer instruction-guidedlocalediting. 2024IEEE/CVFConfer-
Vision(WACV),pages4279–4289,2023. 3 enceonComputerVisionandPatternRecognition(CVPR),
[33] KaimingHe,X.Zhang,ShaoqingRen,andJianSun. Deep pages6254–6263,2023. 2
residuallearningforimagerecognition.2016IEEEConfer- [48] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
enceonComputerVisionandPatternRecognition(CVPR), Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and
pages770–778,2015. 3 C. Lawrence Zitnick. Microsoft coco: Common objects
[34] AmirHertz,RonMokady,JayTenenbaum,KfirAberman, incontext. InEuropeanConferenceonComputerVision,
YaelPritch,andDanielCohen-or.Prompt-to-promptimage 2014. 6,13,15,16,17
editingwithcross-attentioncontrol. InTheEleventhInter- [49] YaronLipman,RickyT.Q.Chen,HeliBen-Hamu,Maxi-
nationalConferenceonLearningRepresentations,2022.2, milianNickel,andMattLe. Flowmatchingforgenerative
6,7,13,14,15,16 modeling. ArXiv,abs/2210.02747,2022. 1
[35] AmirHertz,AndreyVoynov,ShlomiFruchter,andDaniel [50] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow
Cohen-Or.Stylealignedimagegenerationviasharedatten- straight and fast: Learning to generate and transfer data
tion. 2024IEEE/CVFConferenceonComputerVisionand withrectifiedflow. ArXiv,abs/2209.03003,2022. 1
PatternRecognition(CVPR),pages4775–4785,2023. 8 [51] Barak Meiri, Dvir Samuel, Nir Darshan, Gal Chechik,
[36] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdif- Shai Avidan, and Rami Ben-Ari. Fixed-point inver-
fusion probabilistic models. In Proc. NeurIPS, 2020. 1, sion for text-to-image diffusion models. arXiv preprint
3 arXiv:2312.12540,2023. 3
[37] EliahuHorwitz,JonathanKahana,andYedidHoshen. Re- [52] ChenlinMeng,YutongHe,YangSong,JiamingSong,Jia-
coveringthepre-fine-tuningweightsofgenerativemodels. junWu,Jun-YanZhu,andStefanoErmon. Sdedit: Guided
ArXiv,abs/2402.10208,2024. 2 image synthesis and editing with stochastic differential
[38] Nisha Huang, Fan Tang, Weiming Dong, Tong-Yee Lee, equations. In International Conferenceon Learning Rep-
andChangshengXu. Region-awarediffusionforzero-shot resentations,2021. 2,6,7,13,14,15,16
text-drivenimageediting. ArXiv,abs/2302.11797,2023. 2 [53] RonMokady,AmirHertz,KfirAberman,YaelPritch,and
[39] YiHuang,JianchengHuang,YifanLiu,MingfuYan,Jiaxi Daniel Cohen-Or. Null-text inversion for editing real im-
Lv,JianzhuangLiu,WeiXiong,HeZhang,ShifengChen, ages using guided diffusion models. In Proceedings of
andLiangliangCao. Diffusionmodel-basedimageediting: theIEEE/CVFConferenceonComputerVisionandPattern
Asurvey. ArXiv,abs/2402.17525,2024. 2 Recognition,pages6038–6047,2023. 2,3,6,7,13,14,15,
[40] InbarHuberman-Spiegelglas,VladimirKulikov,andTomer 16
Michaeli.Aneditfriendlyddpmnoisespace:Inversionand [54] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
manipulations. arXive-prints,pagesarXiv–2304,2023. 3 Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
[41] JaeseokJeong,JunhoKim,YunjeyChoi,GayoungLee,and MarkChen. Glide: Towardsphotorealisticimagegenera-
YoungjungUh. Visualstylepromptingwithswappingself- tionandeditingwithtext-guideddiffusionmodels. InIn-
attention. ArXiv,abs/2402.12974,2024. 8 ternationalConferenceonMachineLearning,2021. 2
10[55] YotamNitzan,ZongzeWu,RichardZhang,EliShechtman, bach. SDXL:Improvinglatentdiffusionmodelsforhigh-
DanielCohen-Or,TaesungPark,andMichaelGharbi.Lazy resolutionimagesynthesis. ArXiv,abs/2307.01952,2023.
diffusiontransformerforinteractiveimageediting. ArXiv, 2,3
abs/2404.12382,2024. 2 [67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[56] OpenAI. ChatGPT. https://chat.openai.com/, Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
2022. Accessed:2024-10-1. 3,13,21 Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
[57] Maxime Oquab, Timothe´e Darcet, The´o Moutakanni, Krueger, and Ilya Sutskever. Learning transferable visual
HuyQ.Vo,MarcSzafraniec,VasilKhalidov,PierreFernan- modelsfromnaturallanguagesupervision.InInternational
dez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby, ConferenceonMachineLearning,2021. 13,15,21,23
MahmoudAssran,NicolasBallas,WojciechGaluba,Russ [68] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Howes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Chu, and Mark Chen. Hierarchical text-conditional
Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Syn- image generation with CLIP latents. arXiv preprint
naeve,HuijiaoXu,Herve´Je´gou,JulienMairal,PatrickLa- arXiv:2204.06125,2022. 3
batut, Armand Joulin, and Piotr Bojanowski. DINOv2:
[69] Omer Regev, Omri Avrahami, and Dani Lischinski.
Learningrobustvisualfeatureswithoutsupervision.ArXiv,
Click2mask: Localeditingwithdynamicmaskgeneration.
abs/2304.07193,2023. 3,13,15,23
ArXiv,abs/2409.08272,2024. 2
[58] ZhihongPan,RiccardoGherardi,XiufengXie,andStephen
[70] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam
Huang. Effective real image editing with accelerated it-
Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or.
erativediffusioninversion. 2023IEEE/CVFInternational
Encodinginstyle: astyleganencoderforimage-to-image
Conference on Computer Vision (ICCV), pages 15866–
translation. 2021IEEE/CVFConferenceonComputerVi-
15875,2023. 3
sion and Pattern Recognition (CVPR), pages 2287–2296,
[59] GauravParmar,YijunLi,JingwanLu,RichardZhang,Jun-
2020. 3
Yan Zhu, and Krishna Kumar Singh. Spatially-adaptive
[71] DanielRoich,RonMokady,AmitH.Bermano,andDaniel
multilayer selection for gan inversion and editing. 2022
Cohen-Or. Pivotal tuning for latent-based editing of real
IEEE/CVF Conference on Computer Vision and Pattern
images. ACMTransactionsonGraphics(TOG),42:1–13,
Recognition(CVPR),pages11389–11399,2022. 3
2021. 3
[60] GauravParmar,KrishnaKumarSingh,RichardZhang,Yi-
[72] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick
jun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-
Esser, and Bjo¨rn Ommer. High-resolution image synthe-
to-image translation. ACM SIGGRAPH 2023 Conference
siswithlatentdiffusionmodels. 2022IEEE/CVFConfer-
Proceedings,2023. 2
enceonComputerVisionandPatternRecognition(CVPR),
[61] OrPatashnik, ZongzeWu, EliShechtman, DanielCohen-
pages10674–10685,2021. 3
Or, and Dani Lischinski. Styleclip: Text-driven manipu-
[73] OlafRonneberger,PhilippFischer,andThomasBrox. U-
lationofstyleganimagery. 2021IEEE/CVFInternational
net:Convolutionalnetworksforbiomedicalimagesegmen-
ConferenceonComputerVision(ICCV),pages2065–2074,
tation. ArXiv,abs/1505.04597,2015. 1,3
2021. 6,14
[74] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
[62] OrPatashnik,DanielGaribi,IdanAzuri,HadarAverbuch-
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Elor,andDanielCohen-Or. Localizingobject-levelshape
variations with text-to-image diffusion models. 2023 Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-
IEEE/CVF International Conference on Computer Vision mans, etal. Photorealistictext-to-imagediffusionmodels
(ICCV),pages22994–23004,2023. 2 withdeeplanguageunderstanding. AdvancesinNeuralIn-
formationProcessingSystems,35:36479–36494,2022. 3
[63] William S. Peebles and Saining Xie. Scalable diffusion
models with transformers. 2023 IEEE/CVF International [75] Mohammad Salama, Jonathan Kahana, Eliahu Horwitz,
ConferenceonComputerVision(ICCV),pages4172–4182, andYedidHoshen.Datasetsizerecoveryfromloraweights.
2022. 1,2,3 ArXiv,abs/2406.19395,2024. 2
[64] StanislavPidhorskyi, DonaldA.Adjeroh, andGianfranco [76] ShellySheynin,AdamPolyak,UrielSinger,YuvalKirstain,
Doretto. Adversariallatentautoencoders. 2020IEEE/CVF AmitZohar,OronAshual,DeviParikh,andYanivTaigman.
Conference on Computer Vision and Pattern Recognition Emuedit:Preciseimageeditingviarecognitionandgener-
(CVPR),pages14092–14101,2020. 3 ationtasks. ArXiv,abs/2311.10089,2023. 2,6
[65] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aber- [77] Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-
man, Jonathan T. Barron, Amit H. Bermano, Eric Ryan man. Deep inside convolutional networks: Visualising
Chan,TaliDekel,AleksanderHolynski,AngjooKanazawa, image classification models and saliency maps. CoRR,
C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias abs/1312.6034,2013. 3
Nießner, BjornOmmer, Christian Theobalt, PeterWonka, [78] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,
andGordonWetzstein. Stateoftheartondiffusionmodels and Surya Ganguli. Deep unsupervised learning using
forvisualcomputing. ArXiv,abs/2310.07204,2023. 2 nonequilibrium thermodynamics. In International Con-
[66] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, ference on Machine Learning, pages 2256–2265. PMLR,
TimDockhorn,JonasMuller,JoePenna,andRobinRom- 2015. 1
11[79] JiamingSong,ChenlinMeng,andStefanoErmon. Denois- [91] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,
ingdiffusionimplicitmodels. InInternationalConference Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and
onLearningRepresentations,2020. 2,3 MikeZhengShou. Tune-a-video: One-shottuningofim-
[80] Yang Song and Stefano Ermon. Generative modeling by age diffusion models for text-to-video generation. 2023
estimatinggradientsofthedatadistribution. Advancesin IEEE/CVF International Conference on Computer Vision
NeuralInformationProcessingSystems,32,2019. 1 (ICCV),pages7589–7599,2022. 1,4
[81] YoadTewel,RinonGal,DvirSamuelYuvalAtzmon,Lior [92] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue,
Wolf,andGalChechik. Add-it:Training-freeobjectinser- BoleiZhou,andMing-HsuanYang. Ganinversion: Asur-
tioninimageswithpretraineddiffusionmodels,2024. 2 vey. IEEETransactionsonPatternAnalysisandMachine
Intelligence,45:3121–3138,2021. 2
[82] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior
Wolf,GalChechik,andYuvalAtzmon. Training-freecon- [93] ShaoanXie,ZhifeiZhang,ZheLin,TobiasHinz,andKun
sistent text-to-image generation. ArXiv, abs/2402.03286, Zhang. Smartbrush: Textandshapeguidedobjectinpaint-
2024. 8 ingwithdiffusionmodel. 2023IEEE/CVFConferenceon
Computer Vision and Pattern Recognition (CVPR), pages
[83] OmerTov,YuvalAlaluf,YotamNitzan,OrPatashnik,and
22428–22437,2022. 2
DanielCohen-Or.Designinganencoderforstyleganimage
manipulation.ACMTransactionsonGraphics(TOG),40:1 [94] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xue-
–14,2021. 3 jinChen,XiaoyanSun,DongChen,andFangWen. Paint
byexample: Exemplar-basedimageeditingwithdiffusion
[84] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
models. 2023 IEEE/CVF Conference on Computer Vi-
Dekel. Plug-and-play diffusion features for text-driven
sionandPatternRecognition(CVPR),pages18381–18391,
image-to-image translation. In Proceedings of the
2022. 6
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages1921–1930,2023. 2,3 [95] ZhenYang,DinggangGui,WenWang,HaoChen,Bohan
Zhuang,andChunhuaShen.Object-awareinversionandre-
[85] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv
assemblyforimageediting. ArXiv,abs/2310.12149,2023.
Leviathan. Unitune:Text-drivenimageeditingbyfinetun-
3
ing an image generation model on a single image. arXiv
[96] Matthew D. Zeiler and Rob Fergus. Visualizing and un-
preprintarXiv:2210.09477,2022. 2
derstandingconvolutionalnetworks.ArXiv,abs/1311.2901,
[86] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro
2013. 3
Cuenca,NathanLambert,KashifRasul,MishigDavaadorj,
[97] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and
and Thomas Wolf. Diffusers: State-of-the-art diffusion
Yu Su. Magicbrush: A manually annotated dataset for
models. https://github.com/huggingface/
instruction-guided image editing. In Advances in Neural
diffusers,2022. 13,21
InformationProcessingSystems,2023. 2,6,7,13,14,15,
[87] BramWallace,AkashGokul,andNikhilVijayNaik.Edict:
16
Exactdiffusioninversionviacoupledtransformations.2023
[98] RichardZhang,PhillipIsola,AlexeiA.Efros,EliShecht-
IEEE/CVF Conference on Computer Vision and Pattern
man,andOliverWang. Theunreasonableeffectivenessof
Recognition(CVPR),pages22532–22541,2022. 3
deepfeaturesasaperceptualmetric.2018IEEE/CVFCon-
[88] Qian Wang, Biao Zhang, Michael Birsak, and Peter
ferenceonComputerVisionandPatternRecognition,pages
Wonka. Instructedit: Improving automatic masks for
586–595,2018. 13,15,22,23
diffusion-basedimageeditingwithuserinstructions.ArXiv,
[99] JiapengZhu,YujunShen,DeliZhao,andBoleiZhou. In-
abs/2305.18047,2023. 2
domainganinversionforrealimageediting. InEuropean
[89] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi
conference on computer vision, pages 592–608. Springer,
Pont-Tuset,ShaiNoy,StefanoPellegrini,YasumasaOnoe,
2020. 3
SarahLaszlo,DavidJFleet,RaduSoricut,etal.Imagened-
[100] PeihaoZhu,RameenAbdal,YipengQin,andPeterWonka.
itor and editbench: Advancing and evaluating text-guided
Improvedstyleganembedding:Wherearethegoodlatents?
imageinpainting.InProceedingsoftheIEEE/CVFConfer-
ArXiv,abs/2012.09036,2020. 3
ence on Computer Vision and Pattern Recognition, pages
18359–18369,2023. 2
[90] ThomasWolf,LysandreDebut,VictorSanh,JulienChau-
mond, Clement Delangue, Anthony Moi, Pierric Cistac,
Tim Rault, Re´mi Louf, Morgan Funtowicz, Joe Davison,
SamShleifer,PatrickvonPlaten,ClaraMa,YacineJernite,
Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
MariamaDrame,QuentinLhoest,andAlexanderM.Rush.
Transformers:State-of-the-artnaturallanguageprocessing.
InProceedingsofthe2020ConferenceonEmpiricalMeth-
odsinNaturalLanguageProcessing: SystemDemonstra-
tions,pages38–45,Online,2020.AssociationforCompu-
tationalLinguistics. 13
12Stable Flow: Vital Layers for Training-Free Image Editing
Supplementary Material
Acknowledgments. We thank Omer Dahary for his We adapt the text prompts based on the baseline type:
valuable help and feedback. This work was supported in for SDEdit [52], P2P+NTI [34, 53], and MasaCTRL [18],
part by the Israel Science Foundation (grants 1574/21 and we used the standard text prompt describing the desired
2203/24). editedscene(e.g.,“Aphotoofamanwitharedhat”). For
theinstruction-basedbaselinesInstruct-P2P[17]andMag-
A.ImplementationDetails icBrush[97]weadaptedthestyletofitaninstructionalfor-
mat(e.g.,“Makethepersonweararedhat”).
InAppendixA.1,westartbyprovidingimplementationde- We used the following third-party implementations in
tails for our method. Next, in Appendix A.2, we provide thisproject:
the implementation details for the baselines we compared
• FLUX.1-dev model [46] HuggingFace Diffusers [86]
ourmethodagainst.Later,inAppendixA.3,weprovidethe
implementation at https : / / github . com /
implementationdetailsfortheautomaticevaluationsdataset
huggingface/diffusers
and metrics. Finally, in Appendix A.4 we provide the full
• P2P+NTI[34,53]officialimplementationathttps://
detailsoftheuserstudyweconducted.
github.com/google/prompt-to-prompt
• Instruct-P2P [17] official implementation at https:
A.1.MethodImplementationDetails
//github.com/timothybrooks/instruct-
As described in Section 3.1 of the main paper, we started pix2pix
bycollectingadatasetofK =64textpromptsusingChat- • MagicBrush[97]officialimplementationathttps://
GPT[56].Weinstructedittogeneratetextpromptsdescrib- github.com/OSU-NLP-Group/MagicBrush
ing a diverse set of objects in different environments, with • MasaCTRL[18]officialimplementationathttps://
the focus on one main object. Then, we sampled K seeds github.com/TencentARC/MasaCtrl
denoted by S and used them to generate K corresponding • DINOv2 [57] ViT-g/14 implementation by Hugging-
imagesG . Next,foreachlayerl,webypassitbytaking Face Transformers [90] at https://github.com/
ref
only the residual connection values. For each bypass, we huggingface/transformers.
generate K images using the same seed set S demoted by • DINOv1 [19] ViT-B/16 implementation by Hugging-
G . All the images were generated using Euler sampler in Face Transformers [90] at https://github.com/
l
15stepsandaguidancescaleof3.5. huggingface/transformers.
Next, to evaluate the effect of each layer l on the fi- • CLIP [67] ViT-L/14 implementation by Hugging-
nal result, we compared the generated images G with Face Transformers [90] implementation at https://
l
their corresponding images G using the DINOv2 [57] github.com/huggingface/transformers
ref
perceptual similarity metric. We term the layers that ef- • LPIPS [98] official implementation at
fect the generated image the most (i.e., the layers with https : / / github . com / richzhang /
the lowest perceptual similarity) as vital layers, while PerceptualSimilarity.
the rest of the layers as non-vital layers. We found
that the vital layers in the FLUX.1-dev model [46] are A.3.AutomaticMetricsImplementationDetails
[0,1,2,17,18,25,28,53,54,56]. Forvisualizationresults,
As explained in Section 4.1 of the main paper, we prepare
please refer to Appendix B.5. We empirically found that
an evaluation dataset based on the COCO [48] validation
layer 2 can be removed from this set. In addition, the vi-
dataset. We begin by filtering the dataset automatically to
tallayersfortheStableDiffusion3(SD3)[25]modelvital
includeatleastoneprominentnon-rigidbody.Morespecif-
layersare: [0,7,8,9]. Formoredetails,pleaserefertoAp-
ically, wefilteronlyimagescontaininghumansoranimals
pendixB.6.
that at least one of them is prominent enough, but not too
small, i.e., the prominent non-rigid body occupies at least
A.2.BaselinesImplementationDetails
5%oftheimagebutnomorethan33%. Next,foreachim-
AsexplainedinSection4.1ofthemainpaper,wecompare age, we apply various image editing tasks (non-rigid edit-
our method against the following baselines: SDEdit [52], ing,objectaddition,objectreplacement,andsceneediting)
P2P+NTI [34, 53], Instruct-P2P [17], MagicBrush [97], that take into account the prominent object from a list of
and MasaCTRL [18]. We reimplement SDEdit using the differentcombinations,resultinginatotaldatasetof3,200
FLUX.1-dev model [46], and use the official implementa- samples. Examplesofimagesfromthisdatasetcanbeseen
tionfortherestofthebaselines. inFigure13.
13Figure12. UserStudyTrial. Weprovideanexampleofatrial
taskintheuserstudyconductedusingAmazonMechanicalTurk
(AMT)[6]. Userswereaskedfourquestionsofatwo-alternative
forced-choice format. Complete instructions are shown in Fig-
ure11.
Figure 11. User Study Instructions. We provide the complete Table 4. User Study Statistical Significance. A binomial sta-
instructionsfortheuserstudyweconductedusingAmazonMe- tistical test of the user study results suggests that our results are
chanicalTurk(AMT)[6]tocompareourmethodwitheachbase- statisticallysignificant(p-value<5%).
line.
Oursvs PromptAdher. ImagePres. Realism Overall
p-value p-value p-value p-value
We evaluate the editing results using three metrics: (1) SDEdit[52] <1e−8 <1e−8 <1e−6 <1e−8
CLIP which measures the similarity between the in- P2P+NTI[34,53] <1e−8 <1e−8 <1e−8 <6e−8
img
put image and the edited image by calculating the nor- Instruct-P2P[17] <1e−8 <1e−8 <1e−8 <2e−4
MagicBrush[97] <5e−5 <1e−8 <1e−8 <1e−8
malizedcosinesimilarityoftheirCLIPimageembeddings.
MasaCTRL[18] <1e−8 <1e−8 <1e−8 <1e−8
(2) CLIP which measures the similarity between the
txt
edited image and the target editing prompt by calculat-
ingthenormalizedcosinesimilaritybetweentheCLIPim- amples,asexplainedinAppendixA.3.Wecomparedallthe
age embedding and the target text CLIP embedding. (3) baselineswithourmethodusingastandardtwo-alternative
CLIP dir[28,61]whichmeasuresthesimilaritybetweenthe forced-choice format. The users were given full instruc-
directionofthepromptchangeandthedirectionoftheim- tions, as can be seen in Figure 11. Then, for each study
agechange. trial, as shown in Figure 12, users were presented with an
imageandaninstruction“Giventhefollowinginputimage
A.4.UserStudyDetails
of a {CATEGORY}” where {CATEGORY} is the COCO
AsdescribedinSection4.2ofthemainpaper,weconducted categoryoftheprominentobject. Theusersweregiventwo
anextensiveuserstudyusingtheAmazonMechanicalTurk editing results — one from our method and one from the
(AMT)[6]platform,usingautomaticallygeneratedtestex- baseline,andwereaskedthefollowingquestions:
141. “Which of the results is better in adhering to the text Finally,inFigures15and16,wepresentadditionalim-
prompt{PROMPT}?”,where{PROMPT}istheediting ageeditingresultsusingourmethod.
targetprompt.
B.2.DifferentPerceptualMetrics
2. “Whichoftheresultsisbetterinpreservingtheinforma-
tionoftheinputimage?” As explained in Section 3.1 of the main paper, we assess
3. “Whichoftheresultslooksmorerealistic?” theimpactofeachlayerbymeasuringtheperceptualsimi-
4. “Whichoftheresultsisbetterinoverall?” laritybetweenG andG usingDINOv2[57]. Itraisesthe
ref ℓ
Wecollectedfiveratingspersample,resultingin320rat- question of the importance of the specific perceptual [42]
ings per baseline, for a total of 1,920 responses. The time similaritymetricwhendeterminingthevitallayers.
allotted per task was one hour, to allow raters to properly To this end, we also experiment with different percep-
evaluatetheresultswithouttimepressure. Abinomialsta- tualmetrics: DINOv1[19],CLIP[67],andLPIPS[98]. In
tisticaltestoftheuserstudyresults,aspresentedinTable4, Figures18,19and20weplottheperceptualsimilarityper
suggeststhatourresultsarestatisticallysignificant(p-value layerforeachofthesemetrics. Thevitallayers,orderedby
<5%). vitality,asdefinedinEquation1ofthemainpaper,foreach
metricare:
B.AdditionalExperiments • DINOv2—[1,0,2,18,53,28,54,17,56,25].
• DINOv1—[1,0,2,18,53,56,54,25,28,17].
InAppendixB.1,westartbyprovidingadditionalcompar- • CLIP—[2,0,1,18,53,56,54,4,17,3].
isons and results of our method. Then, in Appendix B.2, • LPIPS—[0,1,2,18,17,56,53,54,6,4].
we present experiments on using different perceptual met- As can be seen, the vital set V is equivalent for DI-
rics. Followingthat,inAppendixB.3,wetesttheeffectof NOv2 and DINOv1 (even though there is a disagreement
different sizes for vital layer set. Next, in Appendix B.4, on the order). In addition, all the metrics include the
we provide latent nudging experiments. Furthermore, in set of {1,0,2,18,53,54,17,56} to be included in the vi-
Appendix B.5 we present a full visualization of our layer tal set, while DINOv1 and DINOv2 suggest also includ-
bypassing method. Finally, in Appendix B.6, we test our ing {28,25}, CLIP suggests including {3,4} instead and
methodontheStableDiffusion3backbone. LPIPS suggests including {6,4} instead. In Figure 21 we
edited images with these slightly different vital layer sets,
B.1.AdditionalComparisonsandResults
andfoundthedifferencestobenegligibleinpractice.
InFigure13weprovideanadditionalqualitativecompari-
B.3.NumberofVitalLayers
sonofourmethodagainstthebaselinesonrealimagesex-
tractedfromtheCOCO[48]dataset,asexplainedinSection Thesomewhatagnosticnatureofourmethodtothespecific
4.1 in the main paper. As can be seen, SDEdit [52] strug- perceptualmetric,asdescribedinAppendixB.2,raisesthe
gles with preserving the object identities and backgrounds question of the importance of the entire vital layer set V
(e.g., the bear and chicken examples). P2P+NTI [34, 53] to the editing task. To this end, in Figure 22 we experi-
struggleswithpreservingobjectidentities(e.g.,thebearand mentedwithomittingagrowingnumberofvitallayersand
person examples) and with adding new objects (e.g., the testing the editing results. As can be seen, when remov-
missing hat in the sheep example and missing ball in the ing 80% of the vital layer set, the changes are negligible.
elephantexample). Instruct-P2P[17]andMagicBrush[97] However,whenremovingmorethanthat,theeditingresults
strugglewithnon-rigidediting(e.g.,thepersonraisinghand includeunintendedchanges,suchasidentitychanges(e.g.,
example).MasaCTRL[18]struggleswithpreservingobject man and woman examples) and background changes (e.g.,
identities (e.g., the bear and person examples) and adding cat and blackboard examples). This is consistent with the
newobjects(e.g.,thesheepandcatexamples).Ourmethod, resultsfromAppendixB.2thatshowthattheleastvitallay-
on the other hand, is able to adhere to the editing prompt ersforeachperceptualmetricarelessimportantfortheim-
whilepreservingtheidentities. ageeditingtask.
Next,inFigure14,weprovideaqualitativecomparison
B.4.LatentNudgingExperiments
oftheablatedcasesthatareexplainedin,Section4.3inthe
main paper. As can be seen, we found that (1) perform- AsdescribedinSection3.3ofthemainpaper,weproposed
ingattentioninjectioninallthelayersorperforming(3)an using a latent nudging technique to avoid the bad recon-
attention extension in all the layers, encourages the model struction quality of vanilla inverse Euler ODE solver. We
todirectlycopytheinputimagewhileneglectingthetarget suggest multiplying the initial latent z by a small scalar
0
prompt. Inaddition, (2)performing anattention extension λ = 1.15toslightlyoffsetitfromthetrainingdistribution.
in the non-vital layers or (4) removing the latent nudging AsshowninFigure23,weempiricallytesteddifferentval-
reducestheinputimagesimilaritysignificantly. uesforthelatentnudginghyperparameterλ. Weperformed
15Input SDEdit[52] P2P+NTI[34,53] Instruct-P2P[17] MagicBrush[97] MasaCTRL[18] StableFlow(ours)
“Aphotoofabearwithalonghair”
“Aphotoofamanraisinghishand”
“Aphotoofasheepwithayellowhat”
“Aphotoofacatwearingpurplesunglasses”
“Aphotoofachickenduringsunset”
“Aphotoofanelephantnexttoablueball”
Figure13. BaselinesQualitativeComparisononAutomaticDataset. AsexplainedinSection4.1ofthemainpaper,wecompareour
methodagainstthebaselinesonrealimagesextractedfromtheCOCO[48]dataset. WefindthatSDEdit[52]struggleswithpreserving
theobjectidentitiesandbackgrounds(e.g., bearandchickenexamples). P2P+NTI [34,53]struggleswithpreservingobjectidentities
(e.g., bear and person examples) and with addingnew objects (e.g., missing hat in the sheep example and missing ball in the elephant
example). Instruct-P2P[17]andMagicBrush[97]strugglewithnon-rigidediting(e.g.,personraisinghand). MasaCTRL[18]struggles
withpreservingobjectidentities(e.g.,bearandpersonexamples)andaddingnewobjects(e.g.,sheepandcatexamples). Ourmethod,on
theotherhand,isabletoadheretotheeditingpromptwhilepreservingtheidentities.
16Input (1)Inj.alllayers (2)Inj.non-vitallayers (3)Extensionalllayers (4)w/olatentnudging StableFlow(ours)
“Aphotoofabearwithalonghair”
“Aphotoofamanraisinghishand”
“Aphotoofasheepwithayellowhat”
“Aphotoofacatwearingpurplesunglasses”
“Aphotoofachickenduringsunset”
“Aphotoofanelephantnexttoablueball”
Figure14. AblationsQualitativeComparisononAutomaticDataset. AsexplainedinSection4.3ofthemainpaper,wecompareour
methodagainstseveralablationcasesonrealimagesextractedfromtheCOCO[48]dataset.Ascanbeseen,wefoundthat(1)performing
attentioninjectioninallthelayersorperforming(3)anattentionextensioninallthelayersencouragesthemodeltodirectlycopytheinput
imagewhileneglectingthetargetprompt. Inaddition,(2)performinganattentionextensioninthenon-vitallayersor(4)removingthe
latentnudgingreducestheinputimagesimilaritysignificantly.
17Input “A‘StableFlow’neonsign” “A‘P=NP’neonsign” “Aneonsignofavocados”
Input “Awoodenlion” “Awoodentoilet” “Awoodennoodlesbowl”
Input “Ahedgehog” “Ashark” “Abird”
Input “Jumping” “Sitting” “Puttingitspawonastone”
Figure15.AdditionalResults.Weprovidevariouseditingresultsofourmethod.Thesedifferenteditsaredoneusingthesamevitallayer
set.
18Input “Analbinoporcupine” “Ahorse” “Acrow”
Input “Wearingaredshirt” “Wearingpurplejeans” “Wearingglasses”
Input “Thetext‘FLUX’iswritten “Acamelinthebackground” “Acatinsidethebag”
onthebag”
Input “Apinkcar” “Amandrivingthecar” “Intheevening”
Figure16.AdditionalResults.Weprovidevariouseditingresultsofourmethod.Thesedifferenteditsaredoneusingthesamevitallayer
set.
19Input “Minds” “Think” “Alike”
Figure17. AdditionalResults. Givenaninputimagethatcontainatext,ourmethodcateditthetextwhilekeepingthebackgroundand
style.
200.98
0.96
0.94
0.92
0 5 10 15 20 25 30 35 40 45 50 55 60
LayerIndex
ytiralimiSlautpecrePPILC
0.95
0.9
0.85
0.8
0 5 10 15 20 25 30 35 40 45 50 55 60
LayerIndex
Figure 18. Layer Removal Quantitative Comparison Using
CLIP.AsexplainedinAppendixB.2,wemeasuredtheeffectof
removing each layer of the model by calculating the CLIP [67]
perceptualsimilaritybetweenthegeneratedimageswithandwith-
out this layer. Lower perceptual similarity indicates significant
changesinthegeneratedimages. Ascanbeseen,removingcer-
tain layers significantly affects the generated images, while oth-
ers have minimal impact. Importantly, influential layers are dis-
tributedacrossthetransformerratherthanconcentratedinspecific
regions.Notethatthefirstvitallayerswereomittedforclarity(as
theirperceptualsimilarityapproachedzero).
inversion using the inverse Euler ODE solver with a high
numberof1,000inversion(anddenoising)steps,toreduce
theinversionerror. However,evenwhenusingsuchahigh
number of inversion/denoising steps, we notice that when
notusinglatentnudging(i.e., λ = 1.0), thereconstruction
quality is poor (notice the eyes and the legs of the dog).
Next,wefoundthatλ = 1.15isthesmallestvaluethaten-
ablesfullreconstructionusingtheinverseEulersolver.Fur-
thermore, nudging values that are too high (e.g., λ = 3.0)
resultinsaturatedimages. Lastly,wenoticethatdecreasing
nudgingvalues(i.e.,λ < 1.0)severelydamagestherecon-
structionquality.
Inaddition,weexperimentwithasimplerinversionvari-
ant based on latent caching (termed DDPM bucketing in
DiffUHaul [11]), in which we saved the series of latents
duringtheinversionprocesswithoutapplyinglatentnudg-
ing. AsshowninFigure24,thisapproachindeedachieves
perfect inversion (second column), but (third column) still
struggleswithpreservingtheidentitieswhileeditingtheim-
age(e.g.,therabbitandduckexamples)orsignificantlyal-
ters the image (e.g., the cat and man examples). On the
other hand, our method (fourth column) with the latent
nudgingisabletopreservetheidentitiesduringediting. In
practice, we found that using latent caching in addition to
latent nudging enables inversion with a lower number of
steps(50steps),hence,thisistheapproachweused.
ytiralimiSlautpecreP1vONID
Figure19.LayerRemovalQuantitativeComparisonUsingDI-
NOv1. AsexplainedinAppendixB.2,wemeasuredtheeffectof
removingeachlayerofthemodelbycalculatingtheDINOv1[19]
perceptualsimilaritybetweenthegeneratedimageswithandwith-
out this layer. Lower perceptual similarity indicates significant
changesinthegeneratedimages. Ascanbeseen,removingcer-
tain layers significantly affects the generated images, while oth-
ers have minimal impact. Importantly, influential layers are dis-
tributedacrossthetransformerratherthanconcentratedinspecific
regions.Notethatthefirstvitallayerswereomittedforclarity(as
theirperceptualsimilarityapproachedzero).
B.5.LayerBypassingVisualization
As explained in Section 3.1 of the main paper, to quantify
layerimportanceinFLUXmodel,wedevisedasystematic
evaluation approach. Using ChatGPT [56], we automati-
cally generated a set P of k = 64 diverse text prompts,
and draw a set S of random seeds. Each of these prompts
wasusedtogenerateareferenceimage,yieldingasetG .
ref
For each DiT layer ℓ ∈ L, we performed an ablation by
bypassingthelayerusingitsresidualconnection. Thispro-
cess generated a set of images G from the same prompts
ℓ
andseeds.
InFigures 25–32, we provideafull visualizationof the
referencesetG alongwiththegenerationsetsG –G .
ref 0 56
As can be seen, removing certain layers significantly af-
fectsthegeneratedimages, whileothershaveminimalim-
pact. Importantly, influential layers are distributed across
thetransformerratherthanconcentratedinspecificregions.
B.6.StableDiffusion3Results
All the experiments in the main paper were based on the
FLUX.1-dev[46]model. Wealsoexperimentedwithadif-
ferent DiT text-to-image flow model named Stable Diffu-
sion 3 [25] based on the Diffusers [86] implementation of
themediummodel.
AsdescribedinSection3.1ofthemainpaper,wemea-
211
1
1
1
1
1
0 5 10 15 20 25 30 35 40 45 50 55 60
LayerIndex
ytiralimiSlautpecreP)SPIPL-1(
Figure 20. Layer Removal Quantitative Comparison Using
LPIPS. As explained in Appendix B.2, we measured the ef-
fect of removing each layer of the model by calculating the (1 -
LPIPS) [98] perceptual similarity between the generated images
withandwithoutthislayer. Lowerperceptualsimilarityindicates
significantchangesinthegeneratedimages. Ascanbeseen, re-
moving certain layers significantly affects the generated images,
whileothershaveminimalimpact. Importantly,influentiallayers
aredistributedacrossthetransformerratherthanconcentratedin
specificregions. Notethatthefirstvitallayerswereomittedfor
clarity(astheirperceptualsimilarityapproachedzero).
sured the importance of each of the layers of this model.
As shown in Figure 33, we measured the effect of remov-
ingeachlayerfromthemodelbycalculatingtheperceptual
similarity between the generated images with and without
thislayer. Lowerperceptualsimilarityindicatessignificant
changes in the generated images. As can be seen, remov-
ingcertainlayerssignificantlyaffectsthegeneratedimages,
whileothershaveminimalimpact.
Next, in Figure 34 we illustrate the qualitative differ-
encesbetweenvitalandnon-vitallayers. Whilebypassing
non-vitallayers(G andG )resultsinmodestalterations,
1 21
bypassing vital layers leads to significant changes: com-
plete noise generation (G ) or severe distortions (G , G ,
0 7 8
andG ).
9
Finally,inFigure35,weperformvariouseditingopera-
tions using the same mechanism of injecting the reference
imageinformationintothevitallayersofthemodel,asde-
scribedinSection3.2ofthemainpaper.
22Input DINO[19,57] CLIP[67] LPIPS[98]
“Amanholdingacupoftea”
“Anavocadoatthebeach”
“Ablackboardwiththetext“VitalLayersAreAllYouNeed””
“Theflooriscoveredwithsnow”
Figure21. MetricsQualitativeComparison. AsdescribedinAppendixB.2,wealsoexperimentedwithotherperceptualmetrics. We
foundDINOv2[57]andDINOv1[19]toproducethesamesetofvitallayer. WhileCLIP[67]andLPIPS[98]replacedtwolayersinthe
vitallayersset(thoughtheyincludemostofthevitallayersetasinDINO).Ascanbeseen,thedifferencesbetweenthesesetsarenegligible
wheneditingimages.
23Input 100% 80% 60% 40% 20%
“Amanholdingacupoftea”
“Anavocadoatthebeach”
“Ablackboardwiththetext“VitalLayersAreAllYouNeed””
“Theflooriscoveredwithsnow”
Figure22. NumberofVitalLayersComparison. AsexplainedinAppendixB.3,weexperimentedwithchoosingadifferentportionof
thecalculatedvitallayersetV. Ascanbeseen,whenremoving80%ofthevitallayerset,thechangesarenegligible. However,when
removingmorethanthat,theeditingresultsincludeunintendedchanges,suchasidentitychanges(e.g.,themanandwomanexamples)and
backgroundchanges(e.g.,thecatandblackboardexamples).
24λ=1.0 λ=1.1 λ=1.15 λ=3.0
Input
λ=0.9 λ=0.8 λ=0.7 λ=0.6
Figure23. LatentNudgingValues. AsdescribedinAppendixB.4,weempiricallytesteddifferentvaluesforthelatentnudginghyper-
parameterλ. Inourexperiments,weperformedinversionusingtheinverseEulerODEsolverwithahighnumberof1,000inversion(and
denoising)steps, toreducetheinversionerror. However, evenwhenusingsuchahighnumberofinversion/denoisingsteps, wenotice
thatwhennotusinglatentnudging(i.e.,λ = 1.0),thereconstructionqualityispoor(noticetheeyesandthelegsofthedog). Next,we
foundthatλ=1.15isthesmallestvaluethatenablesfullreconstructionusingtheinverseEulersolver. Furthermore,nudgingvaluesthat
aretoohigh(e.g.,λ = 3.0)resultinsaturatedimages. Lastly,wenoticethatreducingnudgingvalues(λ < 1.0)severelydamagesthe
reconstructionquality.
25Input Reconstruction Cachingonly LatentNudging
“Arabbittoysittingandwearing
pinksocksduringthelateafternoon”
“Thecatisyellingandraisingitspaw”
“Arubberducknexttoapurpleballduringasunnyday”
“Amanwithalonghair”
Figure24. LatentCaching. AsexplainedinAppendixB.4,wealsotestedalatentcachingapproach[11],inwhichwesavedtheseries
oflatentsduringtheinversionprocesswithoutapplyinglatentnudging. Ascanbeseen,thisapproachindeedachievesperfectinversion
(secondcolumn),but(thirdcolumn)stillstruggleswithpreservingtheidentitieswhileeditingtheimage(e.g.,therabbitandduckexamples)
orsignificantlyalterstheimage(e.g.,thecatandmanexamples). Ontheotherhand,ourmethodwiththelatentnudging(fourthcolumn)
isabletopreservetheidentitiesduringediting.
26G G G
ref 0 1
G G G
2 3 4
G G G
5 6 7
Figure25. FullLayerBypassingVisualizationforFlux. Wevisualizetheindividuallayerbypassingstudyweconducted,asdescribed
inAppendixB.5.WestartbygeneratingasetofimagesG usingafixedsetofseedsandprompts.Then,webypasseachlayerℓbyusing
ref
itsresidualconnectionandgeneratethesetofimagesG usingthesamefixedsetofpromptsandseeds.Inthisvisualization,G –G are
ℓ 0 2
vitallayers,whileG –G arenon-vitallayers.
3 7
27G G G
ref 8 9
G G G
10 11 12
G G G
13 14 15
Figure26. FullLayerBypassingVisualizationforFlux. Wevisualizetheindividuallayerbypassingstudyweconducted,asdescribed
inAppendixB.5.WestartbygeneratingasetofimagesG usingafixedsetofseedsandprompts.Then,webypasseachlayerℓbyusing
ref
itsresidualconnectionandgeneratethesetofimagesG usingthesamefixedsetofpromptsandseeds. Inthisvisualization,G –G
ℓ 8 15
areallnon-vitallayers.
28G G G
ref 16 17
G G G
18 19 20
G G G
21 22 23
Figure27. FullLayerBypassingVisualizationforFlux. Wevisualizetheindividuallayerbypassingstudyweconducted,asdescribed
inAppendixB.5.WestartbygeneratingasetofimagesG usingafixedsetofseedsandprompts.Then,webypasseachlayerℓbyusing
ref
itsresidualconnectionandgeneratethesetofimagesG usingthesamefixedsetofpromptsandseeds.Inthisvisualization,G andG
ℓ 17 18
arevitallayers,whileG andG –G arenon-vitallayers.
16 19 23
29G G G
ref 24 25
G G G
26 27 28
G G G
29 30 31
Figure28. FullLayerBypassingVisualizationforFlux. Wevisualizetheindividuallayerbypassingstudyweconducted,asdescribed
inAppendixB.5.WestartbygeneratingasetofimagesG usingafixedsetofseedsandprompts.Then,webypasseachlayerℓbyusing
ref
itsresidualconnectionandgeneratethesetofimagesG usingthesamefixedsetofpromptsandseeds.Inthisvisualization,G andG
ℓ 25 28
arevitallayers,whileG ,G –G andG –G arenon-vitallayers.
24 26 27 29 31
30G G G
ref 32 33
G G G
34 35 36
G G G
37 38 39
Figure29. FullLayerBypassingVisualizationforFlux. Wevisualizetheindividuallayerbypassingstudyweconducted,asdescribed
inAppendixB.5.WestartbygeneratingasetofimagesG usingafixedsetofseedsandprompts.Then,webypasseachlayerℓbyusing
ref
itsresidualconnectionandgeneratethesetofimagesG usingthesamefixedsetofpromptsandseeds. Inthisvisualization,G –G
ℓ 31 39
arenon-vitallayers.
31G G G
ref 40 41
G G G
42 43 44
G G G
45 46 47
Figure30. FullLayerBypassingVisualizationforFlux. Wevisualizetheindividuallayerbypassingstudyweconducted,asdescribed
inAppendixB.5.WestartbygeneratingasetofimagesG usingafixedsetofseedsandprompts.Then,webypasseachlayerℓbyusing
ref
itsresidualconnectionandgeneratethesetofimagesG usingthesamefixedsetofpromptsandseeds. Inthisvisualization,G –G
ℓ 40 47
arenon-vitallayers.
32G G G
ref 48 49
G G G
50 51 52
G G G
53 54 55
Figure31. FullLayerBypassingVisualizationforFlux. Wevisualizetheindividuallayerbypassingstudyweconducted,asdescribed
inAppendixB.5.WestartbygeneratingasetofimagesG usingafixedsetofseedsandprompts.Then,webypasseachlayerℓbyusing
ref
itsresidualconnectionandgeneratethesetofimagesG usingthesamefixedsetofpromptsandseeds. Inthisvisualization,G –G
ℓ 53 54
arevitallayers,whileG –G andG arenon-vitallayers.
48 52 55
33G G
ref 56
Figure32. FullLayerBypassingVisualizationforFlux. Wevisualizetheindividuallayerbypassingstudyweconducted,asdescribed
inAppendixB.5.WestartbygeneratingasetofimagesG usingafixedsetofseedsandprompts.Then,webypasseachlayerℓbyusing
ref
itsresidualconnectionandgeneratethesetofimagesG usingthesamefixedsetofpromptsandseeds.Inthisvisualization,G isavital
ℓ 56
layer.
340.8
0.6
0.4
0.2
0
0 5 10 15 20
LayerIndex
ytiralimiSlautpecreP
Figure 33. Layer Removal Quantitative Comparison Stable
Diffusion3. AsexplainedinAppendixB.6,wemeasuredtheef-
fectofremovingeachlayerofthemodelbycalculatingthepercep-
tualsimilaritybetweenthegeneratedimageswithandwithoutthis
layer.Lowerperceptualsimilarityindicatessignificantchangesin
thegeneratedimages.Ascanbeseen,removingcertainlayerssig-
nificantlyaffectsthegeneratedimages,whileothershaveminimal
impact.Foravisualcomparison,pleaserefertoFigure34.
ferG
0G
1G
7G
8G
9G
12G
Figure34.LayerRemovalQualitativeComparisonStableDif-
fusion3. AsexplainedinAppendixB.6,weillustratethequalita-
tivedifferencesbetweenvitalandnon-vitallayers. Whilebypass-
ing non-vital layers (G and G ) results in modest alterations,
1 21
bypassingvitallayersleadstosignificantchanges:completenoise
generation (G ), or severe distortions (G , G and G ). For a
0 7 8 9
quantitativecomparison,pleaserefertoFigure33
35Input “Adogstatue” “Alemur” “Apig” “Agecko”
Input “Angry” “Closinghiseyes” “Wearingglasses” “Anoldman”
Input “Apinkcar” “Abluecar” “Anorangecar” “Agreencar”
Input “Closingitseyes” “Wearingaredhat” “Wearinggreen “Nexttoapurplestone”
glasses”
Figure35.StableDiffusion3EditingResults.AsexplainedinAppendixB.6,wetestedourStableFlowmethodontheStableDiffusion
3backbone[25]. Ascanbeseen,weareabletoperformvariouseditingoperationsusingthesamemechanismofinjectingthereference
imageinformationintothevitallayersofthemodel.
36